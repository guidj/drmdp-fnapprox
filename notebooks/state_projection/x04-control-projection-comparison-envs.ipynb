{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f83fc8-aefa-44f6-9eb3-7560098ed123",
   "metadata": {},
   "source": [
    "# Prototyping: Control with Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a0c2c-c4f7-43a4-9e94-d7548ba15f88",
   "metadata": {},
   "source": [
    "Our aim is to carry out RL tasks when rewards are delayed (aggregate, and anonymous), using linear function approximation.\n",
    "To solve this problem, we aim to project both large discrete states and continuous states into basis vectors.\n",
    "\n",
    "\n",
    "In this notebook, we test policy control with linear function approximation, using a select set of environments .\n",
    "These environments have either discrete r continuous states, and discrete actions.\n",
    "\n",
    "The environments and their best encoding for reward estimation from previous analyses are:\n",
    "\n",
    "  - GridWorld: Tiling(8, HT=512)\n",
    "  - RedGreen: Tiling(2)\n",
    "  - MountainCar: Tiling(4)\n",
    "  - GEM:Finite-CC-PMSM-v0 (Gym Electric Motor): Scaled raw features\n",
    "\n",
    "We cannot know whether a feature encoding that is suitable for estimating the rewards from aggregate samples is equally adequate to represente state for learning an action-value function in control (nor a state-value in the case of policy evaluation).\n",
    "So, for each, we also use a second encoding as a reference for comparison: \n",
    "\n",
    "  - GridWorld: Random Binary Vectors\n",
    "  - RedGreen: Random Binary Vectors\n",
    "  - MountainCar: Gaussian Mixture(covariance_type='diag', n_components=3)\n",
    "  - GEM:Finite-CC-PMSM-v0 (Gym Electric Motor): GaussianMixture(covariance_type='diag', n_components=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def89d-da64-421c-9fa7-4e7c383cac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d273e-aa6e-40b5-9cd3-e44c87415250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf92d2-67b1-4ae9-a4e7-19209c8d90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drmdp import envs, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e253e23-9aea-4d9a-9263-35e5275ab296",
   "metadata": {},
   "source": [
    "## Control with SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34697e-a7bb-45d2-b3cb-ad5edd1b94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_values(\n",
    "    observation, actions: Sequence[int], weights, feat_transform: feats.FeatTransform\n",
    "):\n",
    "    observations = [observation] * len(actions)\n",
    "    state_action_m = feat_transform.batch_transform(observations, actions)\n",
    "    return np.dot(state_action_m, weights), state_action_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac9ed1-0984-404c-bf43-e5abf82b9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_sarsa(\n",
    "    env, alpha: float, gamma: float, \n",
    "    epsilon: float, num_episodes: int, \n",
    "    feat_transform: feats.FeatTransform,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    weights = np.zeros(feat_transform.output_shape, dtype=np.float64)\n",
    "    returns = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state_qvalues, gradients = action_values(obs, actions, weights, feat_transform)\n",
    "        rewards = 0\n",
    "        # choose action\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.choice(np.flatnonzero(state_qvalues == state_qvalues.max()))\n",
    "\n",
    "        while True:\n",
    "            # greedy            \n",
    "            next_obs, reward, term, trunc, _,  = env.step(action)\n",
    "            rewards += reward\n",
    "            \n",
    "            if term or trunc:\n",
    "                weights = weights + alpha * (reward - state_qvalues[action]) * gradients[action]\n",
    "                break\n",
    "\n",
    "            next_state_qvalues, next_gradients = action_values(next_obs, actions, weights, feat_transform)\n",
    "            \n",
    "            if random.random() <= epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                # greedy\n",
    "                next_action = np.random.choice(np.flatnonzero(next_state_qvalues == next_state_qvalues.max()))\n",
    "\n",
    "            weights = weights + alpha * (\n",
    "                reward + gamma * next_state_qvalues[next_action] - state_qvalues[action]\n",
    "            ) * gradients[action]\n",
    "            obs = next_obs\n",
    "            action = next_action\n",
    "            state_qvalues = next_state_qvalues\n",
    "            gradients = next_gradients\n",
    "        returns.append(rewards)\n",
    "        if verbose and (i+1) % math.floor(num_episodes/5) == 0:\n",
    "            print(\"Episode\", i+1, \"mean returns:\", np.mean(returns))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334326dc-abda-483f-a65c-224089bdc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_interaction_data(env, weights, num_episodes: int, feat_transform):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    buffer = []\n",
    "    returns = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        rewards = 0\n",
    "        steps = []\n",
    "        while True:\n",
    "            state_qvalues, _ = action_values(obs, actions, weights, feat_transform)\n",
    "            action = np.random.choice(\n",
    "                np.flatnonzero(state_qvalues == state_qvalues.max())\n",
    "            )\n",
    "            (\n",
    "                next_obs,\n",
    "                reward,\n",
    "                term,\n",
    "                trunc,\n",
    "                _,\n",
    "            ) = env.step(action)\n",
    "            rewards += reward\n",
    "            steps.append((obs, action, next_obs, reward))\n",
    "            obs = next_obs\n",
    "            if term or trunc:\n",
    "                returns.append(rewards)\n",
    "                break\n",
    "        buffer.append(steps)\n",
    "    return buffer, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896f48c-2ebd-4939-818f-963b7d263a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, weights, num_episodes: int, feat_transform):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    returns = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            state_qvalues, _ = action_values(obs, actions, weights, feat_transform)\n",
    "            action = np.random.choice(\n",
    "                np.flatnonzero(state_qvalues == state_qvalues.max())\n",
    "            )\n",
    "            (\n",
    "                next_obs,\n",
    "                reward,\n",
    "                term,\n",
    "                trunc,\n",
    "                _,\n",
    "            ) = env.step(action)\n",
    "            rewards += reward\n",
    "            obs = next_obs\n",
    "            if term or trunc:\n",
    "                returns.append(rewards)\n",
    "                break\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064f5d0-a26d-4673-b5f3-6e470a1165b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_and_evaluate(\n",
    "    env: gym.Env,\n",
    "    args,\n",
    "    alpha: float = 0.001,\n",
    "    epsilon: float = 0.1,\n",
    "    num_episodes: int = 5000,\n",
    "    gamma: float = 1.0,\n",
    "    turns: int = 5,\n",
    "    eval_max_steps: int = 1000,\n",
    "):\n",
    "    rows = []\n",
    "    for kwargs in args:\n",
    "        print(\"Control with Fn Approx - SARSA:\", kwargs)\n",
    "        for turn in range(turns):\n",
    "            print(\"Turn\", turn + 1)\n",
    "            ft_tfx = feats.create_feat_transformer(env, **kwargs)\n",
    "            print(\"ft-tfx:\", vars(ft_tfx))\n",
    "            weights = semi_gradient_sarsa(\n",
    "                env,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon,\n",
    "                num_episodes=num_episodes,\n",
    "                feat_transform=ft_tfx,\n",
    "            )\n",
    "            buffer, returns = collect_interaction_data(\n",
    "                env,\n",
    "                weights=weights,\n",
    "                num_episodes=20,\n",
    "                feat_transform=ft_tfx,\n",
    "            )\n",
    "            print(\n",
    "                f\"Eval (mean) returns: [min:{np.min(returns)}, mean:{np.mean(returns)}, max:{np.max(returns)}]\"\n",
    "            )\n",
    "            rows.append({\"args\": kwargs, \"buffer\": buffer, \"turn\": turn})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5f61-d2ee-49a8-bd83-260d688e2ed3",
   "metadata": {},
   "source": [
    "### Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f45ef2-85e0-452f-b10e-2f32752d1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\"GridWorld-v0\")\n",
    "df_gridworld = control_and_evaluate(env, [\n",
    "    {\"name\": \"random\", \"enc_size\": 64},\n",
    "    {\"name\": \"tiles\", \"tiling_dim\": 6}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c229fb-c82d-414e-9cc6-52fec78ab666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba78b8-a370-4f17-be38-56216d546e70",
   "metadata": {},
   "source": [
    "### Ice World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561509e-b81a-4c55-a7e6-6d64e820b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\"IceWorld-v0\")\n",
    "df_iceworld = control_and_evaluate(env, [\n",
    "    {\"name\": \"random\", \"enc_size\": 64},\n",
    "    {\"name\": \"tiles\", \"tiling_dim\": 6}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec96568-af4b-4c21-b3dd-ecdd61895ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iceworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02576144-78c8-4140-82a1-a13958bf1bd3",
   "metadata": {},
   "source": [
    "### RedGreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456111a8-449a-491a-879e-459f1bd0b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\"RedGreen-v0\")\n",
    "df_redgreen = control_and_evaluate(env, [\n",
    "    {\"name\": \"random\", \"enc_size\": 32},\n",
    "    {\"name\": \"tiles\", \"tiling_dim\": 6}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2fb45-ea08-45f3-9438-06f5d3503c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redgreen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3008-492e-4fb1-adde-b63b0b956440",
   "metadata": {},
   "source": [
    "### Moutain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd95e93-453a-4aaa-b473-1b85d6df319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\"MountainCar-v0\")\n",
    "df_mountaincar = control_and_evaluate(env, [\n",
    "    {\"name\": \"gaussian-mix\", \"params\": {\"n_components\": int(384/3), \"covariance_type\": \"diag\"}},\n",
    "    {\"name\": \"tiles\", \"tiling_dim\": 6},\n",
    "], alpha=0.01, epsilon=0.2, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fa453-8f69-427a-874d-e230c2ac3863",
   "metadata": {},
   "source": [
    "### GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a3ecc-5db9-4e7c-aee6-6143fa727879",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\"Finite-CC-PMSM-v0\", max_time_steps=5000)\n",
    "df_gem = control_and_evaluate(env, [\n",
    "    {\"name\": \"gaussian-mix\", \"params\": {\"n_components\": 11, \"covariance_type\": \"diag\"}},\n",
    "    {\"name\": \"scale\"},\n",
    "], alpha=0.01, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29918851-d4cd-4120-85b5-c13f36854495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
