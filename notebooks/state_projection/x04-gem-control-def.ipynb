{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f83fc8-aefa-44f6-9eb3-7560098ed123",
   "metadata": {},
   "source": [
    "# Prototyping: GEM Control Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a0c2c-c4f7-43a4-9e94-d7548ba15f88",
   "metadata": {},
   "source": [
    "Our aim is to carry out RL tasks when rewards are delayed (aggregate, and anonymous), using linear function approximation.\n",
    "To solve this problem, we aim to project both large discrete states and continuous states into basis vectors.\n",
    "\n",
    "\n",
    "In this notebook, analyze the structure of the control problem for GEM environments, understanding their value function and agent goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96def89d-da64-421c-9fa7-4e7c383cac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import math\n",
    "import random\n",
    "from typing import Sequence, Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d273e-aa6e-40b5-9cd3-e44c87415250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf92d2-67b1-4ae9-a4e7-19209c8d90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drmdp import envs, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e253e23-9aea-4d9a-9263-35e5275ab296",
   "metadata": {},
   "source": [
    "## Control with SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34697e-a7bb-45d2-b3cb-ad5edd1b94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_values(\n",
    "    observation, actions: Sequence[int], weights, feat_transform: feats.FeatTransform\n",
    "):\n",
    "    observations = [observation] * len(actions)\n",
    "    state_action_m = feat_transform.batch_transform(observations, actions)\n",
    "    return np.dot(state_action_m, weights), state_action_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac9ed1-0984-404c-bf43-e5abf82b9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_sarsa(\n",
    "    env, alpha: float, gamma: float, \n",
    "    epsilon: float, num_episodes: int, \n",
    "    feat_transform: feats.FeatTransform,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    weights = np.zeros(feat_transform.output_shape, dtype=np.float64)\n",
    "    returns = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state_qvalues, gradients = action_values(obs, actions, weights, feat_transform)\n",
    "        rewards = 0\n",
    "        # choose action\n",
    "        if random.random() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.choice(np.flatnonzero(state_qvalues == state_qvalues.max()))\n",
    "\n",
    "        while True:\n",
    "            # greedy            \n",
    "            next_obs, reward, term, trunc, _,  = env.step(action)\n",
    "            rewards += reward\n",
    "            \n",
    "            if term or trunc:\n",
    "                weights = weights + alpha * (reward - state_qvalues[action]) * gradients[action]\n",
    "                break\n",
    "\n",
    "            next_state_qvalues, next_gradients = action_values(next_obs, actions, weights, feat_transform)\n",
    "            \n",
    "            if random.random() <= epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                # greedy\n",
    "                next_action = np.random.choice(np.flatnonzero(next_state_qvalues == next_state_qvalues.max()))\n",
    "\n",
    "            weights = weights + alpha * (\n",
    "                reward + gamma * next_state_qvalues[next_action] - state_qvalues[action]\n",
    "            ) * gradients[action]\n",
    "            obs = next_obs\n",
    "            action = next_action\n",
    "            state_qvalues = next_state_qvalues\n",
    "            gradients = next_gradients\n",
    "        returns.append(rewards)\n",
    "        if verbose and (i+1) % math.floor(num_episodes/5) == 0:\n",
    "            print(\"Episode\", i+1, \"mean returns:\", np.mean(returns))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896f48c-2ebd-4939-818f-963b7d263a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, weights, num_episodes: int, feat_transform):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    returns = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            state_qvalues, _ = action_values(obs, actions, weights, feat_transform)\n",
    "            action = np.random.choice(np.flatnonzero(state_qvalues == state_qvalues.max()))\n",
    "            next_obs, reward, term, trunc, _,  = env.step(action)\n",
    "            rewards += reward\n",
    "            obs = next_obs\n",
    "            if term or trunc:\n",
    "                returns.append(rewards)\n",
    "                break\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40729fd0-8514-4df9-a990-da6a0ca173fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_interaction_data(env, weights, num_episodes: int, feat_transform):\n",
    "    actions = tuple(range(env.action_space.n))\n",
    "    buffer = []\n",
    "    returns = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        rewards = 0\n",
    "        steps = []\n",
    "        while True:\n",
    "            state_qvalues, _ = action_values(obs, actions, weights, feat_transform)\n",
    "            action = np.random.choice(np.flatnonzero(state_qvalues == state_qvalues.max()))\n",
    "            next_obs, reward, term, trunc, _,  = env.step(action)\n",
    "            rewards += reward\n",
    "            steps.append(\n",
    "                (obs, action, next_obs, reward)\n",
    "            )\n",
    "            obs = next_obs\n",
    "            if term or trunc:\n",
    "                returns.append(rewards)\n",
    "                break\n",
    "        buffer.append(steps)\n",
    "    return buffer, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064f5d0-a26d-4673-b5f3-6e470a1165b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_and_evaluate(\n",
    "    env: gym.Env,\n",
    "    ft_tfx_args: Sequence[Mapping],\n",
    "    alpha: float = 0.01,\n",
    "    epsilon: float = 0.2,\n",
    "    num_episodes: int = 5000,\n",
    "    gamma: float = 1.0,\n",
    "    turns: int = 5,\n",
    "    eval_episodes: int = 15,\n",
    "    eval_max_steps: int = 1000,\n",
    "):\n",
    "    rows = []\n",
    "    config = {\n",
    "        \"alpha\": alpha,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"num_episodes\": num_episodes, \n",
    "        \"gamma\": gamma,\n",
    "        \"eval_episodes\": eval_episodes,\n",
    "        \"eval_max_steps\": eval_max_steps   \n",
    "    }            \n",
    "    for kwargs in ft_tfx_args:\n",
    "        print(\"Control with Fn Approx - SARSA:\", kwargs)\n",
    "        for turn in range(turns):\n",
    "            print(\"Turn\", turn + 1)\n",
    "            ft_tfx = feats.create_feat_transformer(env, **kwargs)\n",
    "            print(\"ft-tfx:\", vars(ft_tfx))\n",
    "            weights = semi_gradient_sarsa(\n",
    "                env,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon,\n",
    "                num_episodes=num_episodes,\n",
    "                feat_transform=ft_tfx,\n",
    "            )\n",
    "            buffer, returns = collect_interaction_data(\n",
    "                env,\n",
    "                weights=weights,\n",
    "                num_episodes=eval_episodes,\n",
    "                feat_transform=ft_tfx,\n",
    "            )\n",
    "            print(f\"Eval (mean) returns: [min:{np.min(returns)}, mean:{np.mean(returns)}, max:{np.max(returns)}]\")\n",
    "            \n",
    "            rows.append(\n",
    "                {\"ft_tfx_args\": kwargs, \"buffer\": buffer, \"turn\": turn, \"exp_args\": config}\n",
    "            )\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fa453-8f69-427a-874d-e230c2ac3863",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb6b92-0c31-4917-ba43-247a728e64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_trajectories(buffer):\n",
    "    rows = []\n",
    "    for ep, steps in enumerate(buffer):\n",
    "        episode_rows = [\n",
    "            {\"step\": idx, \"reward\": step[3], \"episode\": ep}\n",
    "            for idx, step in enumerate(steps)\n",
    "        ]\n",
    "        rows.extend(episode_rows)\n",
    "    df_plot = pd.DataFrame(rows)\n",
    "    _, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.lineplot(df_plot, x=\"step\", y=\"reward\", hue=\"episode\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030ed7f-622e-4620-88f3-ad1de3dedb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_config_reward_trajectories(df_results, header_config_keys):\n",
    "    configs = np.unique(df_results[\"config_id\"])\n",
    "    num_configs = len(configs)\n",
    "    _, axes = plt.subplots(ncols=num_configs, figsize=(6*num_configs, 6), sharex=True, sharey=True)\n",
    "    \n",
    "    \n",
    "    for config_id, ax in zip(configs, axes.flatten()):\n",
    "        df_slice = df_results[df_results[\"config_id\"] == config_id]\n",
    "        rows = []\n",
    "        # different turns\n",
    "        for row in df_slice.to_dict(\"records\"):\n",
    "            buffer = row[\"buffer\"]\n",
    "            for ep, steps in enumerate(buffer):\n",
    "                episode_rows = [\n",
    "                    {\"step\": idx, \"reward\": step[3], \"episode\": ep, \"turn\": row[\"turn\"]}\n",
    "                    for idx, step in enumerate(steps)\n",
    "                ]\n",
    "                rows.extend(episode_rows)\n",
    "        df_plot = pd.DataFrame(rows)\n",
    "        exp_args = df_slice.iloc[0][\"exp_args\"]\n",
    "        headers = {key: value for key, value in exp_args.items() if key in header_config_keys}\n",
    "        sns.lineplot(df_plot, x=\"step\", y=\"reward\", hue=\"turn\", ax=ax)\n",
    "        ax.set_title(f\"Config: {headers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e37ccb-7802-486c-b9b2-cc98b9aa27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gem_experiments(env, ft_tfx_args, configs):\n",
    "    dfs = []\n",
    "    for config_id, config in enumerate(configs):\n",
    "        df_config = control_and_evaluate(env, ft_tfx_args=ft_tfx_args, **config)\n",
    "        df_config[\"config_id\"] = config_id\n",
    "        dfs.append(df_config)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a873f93-f326-49b0-ac6a-f63ba1d0529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.make(\n",
    "    \"Finite-CC-PMSM-v0\", pos_enforcement=False, constraint_violation_reward=None, \n",
    "    penalty_gamma=0.9, max_episode_steps=5000\n",
    ")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4755261-8b36-4aad-af68-eb57d43dffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cb79f-6392-4dbd-9b06-0f81da66051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c46ee8-9d27-496e-8b7d-d5245d19c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tfx = feats.create_feat_transformer(env, **{\"name\": \"scale\"})\n",
    "ft_tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195c3a8-e471-4727-b5c4-f350fb15fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal weights for random policy\n",
    "weights = np.zeros(ft_tfx.output_shape)\n",
    "buffer, returns = collect_interaction_data(\n",
    "    env,\n",
    "    weights=weights,\n",
    "    num_episodes=20,\n",
    "    feat_transform=ft_tfx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c5c4d-3002-4c79-8d65-6c546ad4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot([exp[3] for exp in buffer[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47211bee-95cd-43d4-96b7-05a896d98287",
   "metadata": {},
   "source": [
    "### No penalty - Varying learning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b6439-5b31-442b-8ab6-12151f3a775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_tfx_args = [\n",
    "    {\"name\": \"scale\"},\n",
    "]\n",
    "env = envs.make(\n",
    "    \"Finite-CC-PMSM-v0\", pos_enforcement=False, penalty_gamma=1.0, \n",
    "    constraint_violation_reward=0.0, max_episode_steps=5000\n",
    ")\n",
    "configs = [\n",
    "    {\"num_episodes\": 1000, \"turns\": 5},\n",
    "    {\"num_episodes\": 2000, \"turns\": 5},\n",
    "    {\"num_episodes\": 4000, \"turns\": 5},\n",
    "]\n",
    "df_nopenalty_vsteps = gem_experiments(env, ft_tfx_args=tf_tfx_args, configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e03e9f4-d30f-4e95-bb61-4b04daabce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nopenalty_vsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63adf06-a63a-4204-9cb2-6cf7e2500b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_config_reward_trajectories(df_nopenalty_vsteps, header_config_keys=(\"num_episodes\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ba140-0e2e-45ec-9688-46a81c64a715",
   "metadata": {},
   "source": [
    "### Discount based penalty, discounted and undiscounted learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189de504-190a-4713-86c2-9b535b512699",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_tfx_args = [\n",
    "    {\"name\": \"scale\"},\n",
    "]\n",
    "env = envs.make(\n",
    "    \"Finite-CC-PMSM-v0\", pos_enforcement=False, penalty_gamma=0.9,\n",
    "    max_episode_steps=5000\n",
    ")\n",
    "configs = [\n",
    "    {\"gamma\": 0.9, \"turns\": 5},\n",
    "    {\"gamma\": 1.0, \"turns\": 5},\n",
    "]\n",
    "df_discountpenalty_vgamma = gem_experiments(env, ft_tfx_args=tf_tfx_args, configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d40d68-e02d-4369-b834-b8be171a0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discountpenalty_vgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91662357-d2f0-4f78-893c-09367e2b9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_config_reward_trajectories(df_discountpenalty_vgamma, header_config_keys=(\"gamma\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed7fdd-a7c6-48c2-8864-82a6e6e2925f",
   "metadata": {},
   "source": [
    "### Fixed penalty, discounted and undiscounted learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70abe9f-ad6b-43a1-8229-975c32e1b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tfx_args = [\n",
    "    {\"name\": \"scale\"},\n",
    "]\n",
    "env = envs.make(\n",
    "    \"Finite-CC-PMSM-v0\", pos_enforcement=False,\n",
    "    constraint_violation_reward=-10, max_episode_steps=5000\n",
    ")\n",
    "configs = [\n",
    "    {\"gamma\": 0.9, \"turns\": 5, \"num_episodes\": 100},\n",
    "    {\"gamma\": 1.0, \"turns\": 5, \"num_episodes\": 100},\n",
    "]\n",
    "df_fixedpenalty_vgamma = gem_experiments(env, ft_tfx_args=tf_tfx_args, configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75a866-1fe4-4ef1-a370-be309be99f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fixedpenalty_vgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed736617-d2c9-4adc-80e6-0a959a30201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_config_reward_trajectories(df_fixedpenalty_vgamma, header_config_keys=(\"gamma\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f25dba-dd9b-47ad-a545-03c3cba95dd4",
   "metadata": {},
   "source": [
    "### Time limit and Longevity based penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd0fd7-06c0-4027-ae33-74979f495b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(1_000_000)\n",
    "# Reward = c1 + c2\n",
    "# c1 is the base reward\n",
    "# c2 is the penalty or incentive to keep going\n",
    "# positive fraction of the reward range?\n",
    "worst_reward = -1.0\n",
    "c_options = [0.01, 0.1, 1, 10, 2*np.abs(worst_reward)]\n",
    "gamma_options = [0.8, 0.9, 0.99, 0.999, 1.0]\n",
    "\n",
    "rows = []\n",
    "for c in c_options:\n",
    "    for gamma in gamma_options:\n",
    "        c2 = c * np.sum(np.power(gamma*np.ones_like(steps), steps))\n",
    "        returns_c1_lb = np.sum(np.power(gamma *np.ones_like(steps), steps) * worst_reward)\n",
    "        rows.append(\n",
    "            {\"c\": c, \"gamma\": gamma, \"returns_c2\": c2, \"returns_c1_lb\": returns_c1_lb}\n",
    "        )\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434eb02-ecae-45d5-9f34-dfe093a8f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9**100, 1/(1-0.9)\n",
    "0.9**1000, 1/(1-0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf67dfa-ebfe-4ce1-a561-6b81211452e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
